Using 'droptol' set to 1e-3, the below number of iterations were observed for the different algorithms:

Method 								 		                                          Number of Iterations
Method of Steepest Descent (no preconditioning)			 		            68
Preconditioned Method of Steepest Descent (incomplete Cholesky)			56
CG (no preconditioning)								                              24
PCG (incomplete Cholesky)							                              19

The number of iterations in the Method of Steepest being 68 is consistent with the fact that unless the matrix A is well-conditioned, the algorithm may keep bouncing back and forth and takes a lot of iterations to converge to a minimum. Preconditioning it with a matrix M (in this case, incomplete Cholesky factorization of A) that resembles A brings it closer to resembling an Identity matrix which is an example of a well-conditioned matrix. The smaller the value of 'droptol', the more M resembles A. Using a 'droptol' of 1e-3, the preconditioned Method of Steepest Descent reduced the number of iterations to 56 (from 68 in the method without preconditioning).

Conjugate Gradient Descent benefits from using A-conjugate search directions such that the next search direction is the minimum in the span of all the previous search directions, instead of restricting x to be on the union of lines defined by x_(j) + alpha*p_(j) (where j=0,..,k) as done in the Method of Steepest Descent. Hence, it takes close to half the number of iterations taken by the preconditioned Method of Steepest Descent. Preconditioning the Conjugate Gradient Descent method accelerates its performance as expected and brings the number of iterations to convergence to 19 (less than half the number of iterations taken by the preconditioned Method of Steepest Descent).
